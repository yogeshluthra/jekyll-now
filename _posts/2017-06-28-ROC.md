---
layout: post
title: ROC
markdown: true
---
{% include math.html %}
# How to evaluate a Classifier using ROC curves

For classification tasks, Receiver Operating Characteristic (ROC) curve and Area Under Curve (AUC) are important concepts to know. At times, I have seen people unclear about these concepts and hence my effort to explain these.

## Why is this important?

Imagine a dataset repesenting 2 classes but highly skewed, say 90%-10% for positive-negative labels. On such dataset, even a random classifier could be 90% accurate.
- How do we evaluate or rank various classifiers performance on such dataset?

Now imagine a multi-class dataset with 100 classes, and there are few classes which makeup up <1% in this set.
- How do we know which of these rare classes are being mis-predicted?
There may be classifiers which probably have lower accuracy than others but may do a good job in correctly classifying such hard examples.
- Would you prefer those?
- How would you trade-off accuracy VS correct-prediction of some of these rare classes.

Answering these questions require some interpretation of how well a classifier has separated different classes.

## Receiver Operating Characteristic basics.

(For purpose of illustrations, I will mainly assume that classifier learns a gaussian distribution for classes, although following analysis can be extended to any distribution. A Gaussian (or Normal) Probability Density Function =$$ N(mean, std) = \frac{1}{\sqrt{2*\pi*std^2}} * e^{\frac{-1}{2} * (\frac{x-mean}{std})^2} $$

Wikipedia describes ROC as: “In statistics, a receiver operating characteristic curve, i.e. ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.”

Discrimination threshold is also sometimes called the Decision Boundary, the meaning of which is as follows.  
Consider 2 Gaussian Distributions learned by our classifier  
![alt text]({{site.url}}/images/2_gaussians.png "2 gaussians"){:height="200px" width="200px"}  
Here, the Blue curve is distribution of 'Positive (or +ve)' examples learnt by our classifier (so Red is 'Negative (or -ve)' distribution learnt).  

Let us say, we set the Decision Boundary (DB)=0.0. That is, if output of classifier is<DB, it will be classified -ve, otherwise +ve.  
![alt text]({{site.url}}/images/tp.png "True Positive"){:height="200px" width="200px"}  
Shaded area under the curve above represents chance that the example belongs to +ve class (Blue curve) and is classified correctly (right to Decision Boundary). This is called True Positive (TP).  
Shaded area can be calculated as:  
$$TP=I(x)=\int_{x}^{\infty} \frac{1}{\sqrt{2*\pi*std^2}} * e^{\frac{-1}{2} * (\frac{y-mean}{std})^2} dy$$  
(here=0.91 that is, 91% actual positive examples are classified correctly. :-))  

It is not hard to understand that Area under Blue curve on the other side of Decision Boundary would be called False Negative, as it is classified -ve (left of decision boundary) but actually belongs to +ve class.  
![alt text]({{site.url}}/images/fn.png "False Negative"){:height="200px" width="200px"}  
This is simply, FN=1-TP  
(here=0.09, that is, only 9% of actual positive examples are classified incorrectly. Still :-))  

So far so good. Does +ve class distribution (Red curve) learnt by our classifier has any effect?  
Shaded area under the curve above represents chance that the example belongs to -ve class (Red curve) and is classified incorrectly (right to Decision Boundary). This is called False Positive (FP).  
![alt text]({{site.url}}/images/fp.png "False Positive"){:height="200px" width="200px"}  
It can be calculated similar to TP but applied on Red curve now.  
(here=0.69. WOW, too many -ve examples classified as +ve. That doesn't sound good :-()  

Further shaded area under the curve above represents chance that the example belongs to -ve class (Red curve) and is classified correctly (left to Decision Boundary). This is called True Negative (TN).  
![alt text]({{site.url}}/images/tn.png "True Negative"){:height="200px" width="200px"}  
Its value can be calculated similar to FN.  
(here=0.31. So -ve classe is not learnt as well as +ve  :-()  

Having defined TP, FN, FP and TN, we can visualize their relationship in a tabular format.  
 ----------------- | --------------- | --------------- |  
                   | Actual Positive | Actual Negative |  
 ----------------- |:---------------:|:---------------:|  
Predicted Positive | True Positive   | False Positive  |  
 ----------------- | --------------- | --------------- |  
Predicted Negative | False Negative  | True Negative   |  
 ----------------- | --------------- | --------------- |  

In order to measure how relevant the predictions of our classifier are, we further calculate Precision and Recall as defined below:  
$$
Precision = \frac{TP}{TP+FP}\\
Recall = \frac{TP}{TP+FN}  
$$  
Basically:  
Precision measures how many relevant +ve examples (TP) are retreived among total retreived +ve examples (TP+FP)  
Recall measures how many relevant +ve example (TP) are retreived among total relevant +ve examples (TP+FN)  
