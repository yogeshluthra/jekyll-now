---
layout: post
title: ROC
markdown: true
---
{% include math.html %}
# How to evaluate a Classifier using ROC curves

For classification tasks, Receiver Operating Characteristic (ROC) curve and Area Under Curve (AUC) are important concepts to know. At times, I have seen people unclear about these concepts and hence my effort to explain these.

## Why is this important?

Imagine a dataset repesenting 2 classes but highly skewed, say 90%-10% for positive-negative labels. On such dataset, even a random classifier could be 90% accurate.
- How do we evaluate or rank various classifiers performance on such dataset?

Now imagine a multi-class dataset with 100 classes, and there are few classes which makeup up <1% in this set.
- How do we know which of these rare classes are being mis-predicted?
There may be classifiers which probably have lower accuracy than others but may do a good job in correctly classifying such hard examples.
- Would you prefer those?
- How would you trade-off accuracy VS correct-prediction of some of these rare classes.

Answering these questions require some interpretation of how well a classifier has separated different classes.

## Receiver Operating Characteristic basics.

(For purpose of illustrations, I will mainly assume that classifier learns a gaussian distribution for classes, although following analysis can be extended to any distribution. A Gaussian (or Normal) Probability Density Function =$$ N(mean, std) = \frac{1}{\sqrt{2*\pi*std^2}} * e^{\frac{-1}{2} * (\frac{x-mean}{std})^2} $$

Wikipedia describes ROC as: “In statistics, a receiver operating characteristic curve, i.e. ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.”

Discrimination threshold is also sometimes called the Decision Boundary, the meaning of which is as follows.  
Consider 2 Gaussian Distributions learned by our classifier
![alt text]({{site.url}}/images/2_gaussians.png "2 gaussians"){:height="200px" width="200px"}
Here, the Blue curve is distribution of 'Positive (or +ve)' examples learnt by our classifier (so Red is 'Negative (or -ve)' distribution learnt).

Let us say, we set the Decision Boundary (DB)=0.0. That is, if output of classifier is<DB, it will be classified -ve, otherwise +ve.  
![alt text]({{site.url}}/images/tp.png "True Positive"){:height="200px" width="200px"}
Shaded area under the curve above represents chance that the example belongs to +ve class (Blue curve) and is classified correctly (right to Decision Boundary). This is called True Positive (TP).
Shaded area can be calculated as:  
$$TP=I(x)=\int_{x}^{\infty} \frac{1}{\sqrt{2*\pi*std^2}} * e^{\frac{-1}{2} * (\frac{y-mean}{std})^2} dy$$
(here=0.91 that is, 91% actual positive examples are classified correctly. :-))

It is not hard to understand that Area under Blue curve on the other side of Decision Boundary would be called False Negative, as it is classified -ve (left of decision boundary) but actually belongs to +ve class.
![alt text]({{site.url}}/images/fn.png "False Negative"){:height="200px" width="200px"}
This is simply, FN=1-TP
(here=0.09, that is, only 9% of actual positive examples are classified incorrectly. Still :-))

So far so good. Does +ve class distribution (Red curve) learnt by our classifier has any effect?  
For this consider Area under the Red curve to the right of decision boundary. 
![alt text]({{site.url}}/images/fp.png "False Positive"){:height="200px" width="200px"}
Shaded area under the curve above represents chance that the example belongs to -ve class (Red curve) and is classified incorrectly (right to Decision Boundary). This is called False Positive (FP).
It can be calculated similar to TP but applied on Red curve now.
(here=0.69. WOW, too many -ve examples classified as +ve. That doesn't sound good :-()

One last area left is under the Red curve to the left of Decision Boundary.
![alt text]({{site.url}}/images/tn.png "True Negative"){:height="200px" width="200px"}
Shaded area under the curve above represents chance that the example belongs to -ve class (Red curve) and is classified correctly (left to Decision Boundary). This is called True Negative (TN).
Its value can be calculated similar to FN. 
(here=0.31. So -ve classe is not learnt as well as +ve  :-()



